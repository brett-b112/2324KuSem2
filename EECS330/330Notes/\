 *Notes*

--------------------------
# Wednesday January 17th
--------------------------

# What problems can algoriths solve?
* #### Managing big data on the internet
    * Search Enginge for findign data

# Why Study Algorithms ?
* #### Algorithm is a technology you should master
    * advacned computer artitecture
    * efficient algoirthms
    * machine learning
    * mobile devices
    * fast networking both wried and wireless
* ##### Consider sorting the list of n = 10^7 numbers as an example
    * #### **Insertion Sort** takes time C1N^2 where c1 is a constant
    * Suppose this is run on a fast computer that can do a numver of instructions
    * Suppose the code is optimized such taht c1 = 2
    (2* (10^7)^2 instructions) = 20000 instructions
* #### **Merge sort** takes time C1N logn where c1 is a constant
    * Considering this running on slow computer it would only take 1163 making it more optimized for the solution


# Friday January 19th

------------------------

# Sorting

* ### Problem of sorting
    * input: an array of numbers
    * Output: a permutation or reordering
        * Example:
            * Input: 8 2 4
            * output: 2 4 8
* ### Why is sorting important
    * sorting can be used to represent important problems and prprocess data for things like travel websites
* ### Sorting Algorithms
    * Insertion, Merge, Quicksort, Heapsort

* # Insertion Sort
*I believe that all of the content and pseudocode has the first index as 1 and not 0**
* https://www.youtube.com/watch?v=JU767SDMDvA&ab_channel=MichaelSambol
    * An efficient algo for sorting a small number of elements
    * **IDEA** similar to sorting a hand of playing cards
        * start with empty left hand and a pile of cards
        * grab from pile with right hand and insert into left hand in correct position with largest num to the right
    * 5 2 4 6 1 3
    * Look at the next position and swap
        * i.e. compare 5 with 2, 2 is less so swap
        * i.e. compare 5 with 4, 4 is less so swap
        * continue until sorted
 #### Psuedcode
    for i = 2n
        key = A[i]
        // insert A[i] int othe sorted subarray A[1:i-1]
        j = i - 1
        while
            J > 0 and A[j] > key
            A[j+i] = A[j]
            j = j - 1
        A[j+1] = key
* ### Why insertion sort works correctly?
    * **Loop invariant**
        * At the start of each iteration of the outer-for loop, the subarray A[1: i-1] consits of the elements orginially in A[1: i-1] but in the sorted order.
    * Need to show three things
        * **Intialization** : true prior to first interation
        * **Maintenance**: true before iteration and remains true before the next iteration
        * **Termination** : loop terminates and invariant shows it's true
* ### Running time analysis
    * denote input size n with T(n)
        * **size of the input n** : short easier to sort
        * **nature of the input** : an already sorted list is easier to sort no shit
    * Time analyzed on RAM model:
        * k-th line of code takes ck time to run where ck is a constant
    * ![Runtime](insertionRuntime.png)
    * **Best Case** : array is already sorted so ti = 1 for i = 2,3, ...... n
    * **Worst Case** :  array is in reverse sorted order so ti = i for i  = 2,3, .... n

* ### Worst-cast running time
    *we'll focus on worst*
    * It gives upper bound providing a runtime **gauruntee* certified by professor sun
    * Worst happens a lot i.e. searching for info on database that doesn't exist
    * The *average case* i.e. one that is half sorted half not is usually just as bad
* ###  Order of Growth
    * We simplify our runtime
    * we remove the leadin term i.e 3* O(n) we remove the 3 it's inconsequential in the long run
    * O(n) faster than O(n^2) regardless of numerical coefficient

--------------------------
# Wenesday January 24th
--------------------------
# More sorting algos (merge sort)
--------------------------

* ## Divide and Conquer
    * Recursion and three problem sets
        * *Divide*
            * The problem into further subproblems smaller than instances of the same problem
        * *Conquer*
            * subproblems are solved recursively
        * *Combine*
            * combine subsolutions to original problem
* ## Merge sort
    * uses divide and conquer
        * ![merge sort](mergeSort.png)
        * ![pseudocode](mergeSortCode.png)
        * merge sort breaks up and array into two partitions, and then each two into two, etc. until there is only one left in each partition
        * next each element is compared against the first element of the other subarray and so on until it is reorganized in order.

![runtime](msRuntime.JPG)
* we ignore second term to get run time.
## SUMMARY
    Insertion Sort: O(n^2)
    Merge Sort: O(n log n)

    Insertion sort is **asymptotically** (ignoring constants) slower than merge sort, ti is faster for sorting small arrays in practice (due to its smaller constant factors)
    Insertion sort is used when there are sufficently small partitions from merge

--------------------------
# Friday January 26th
--------------------------
# Asymptotic Notations
--------------------------
*Asymptotic efficiency* is the worst case running time where the input size in the limit (where higher order dominates and lower order are less relevant)
### O-Notation
* characterizes an upper bound on the asymptotic behavior of the function
* *grows no faster than a certain rate* (again based on highest order)
* ex: f(x) = 7x^3 + 2x^2 + 3 ===> is O(n^3)
### Î©-notation
* Î©-notation characterizes a *lower bound* as the asyptotic  behavior of the function
* the function grows **as least as fast as** a certain rate
* also based on highesdt order
* ğ‘“(ğ‘›) = 7ğ‘›3 + 100ğ‘›2 âˆ’ 20ğ‘› + 6 is Î©(ğ‘›3), since the highest-order term 7ğ‘›3 grows at least as fast as ğ‘›3
### Î˜-notation
* is a **tight bound**
* function grows **precisely** at a certain rate
* again based on highest order
* ğ‘“(ğ‘›) = 7ğ‘›3 + 100ğ‘›2 âˆ’ 20ğ‘› + 6 is both ğ‘‚ ğ‘›3 and Î©(ğ‘›3), so it is also Î˜(ğ‘›3).
* Note: If a function is both ğ‘‚(ğ‘“(ğ‘›)) and Î©(ğ‘“(ğ‘›)), then the function is Î˜(ğ‘“(ğ‘›)).
* proof for insertion sort is **IN THE SLIDES**

##### Insertion sort
* Since INSERTION-SORT has a worst-case running time that is both ğ‘‚(ğ‘›2) and Î©(ğ‘›2), we can conclude that its worst-case running time is Î˜(ğ‘›2).

#### O-notation proof and other content
**CHECK SLIDES OFR FORMAL IMAGES AND EXAMPLES OF NOTATIONS**
* ![proof](proofONotation.JPG)
* other proofs are similar to this one
* O f(n) is less than cg(n)
* Î© f(n) is more than cg(n)
* Î˜ f(n) is sandwiched between c1g(n) and c2g(n)

--------------------------
# Monday January 29th
--------------------------
# Asymptotic Notations
--------------------------
* *Need to be careful when using asymptotic notation to be the most precise that you can be*
* You can say: The worst-case running time for insertion sort is ğ‘‚(ğ‘›2), Î©(ğ‘›2), and Î˜(ğ‘›2); all are correct. Prefer to say ğ›©(ğ‘›2) here, since itâ€™s the most precise.
* You can say: The best-case running time for insertion sort is ğ‘‚(ğ‘›), Î©(ğ‘›), and Î˜ ğ‘› . Prefer ğ›© ğ‘› , again more precise.
* You cannot say: The running time for insertion sort is Î˜(ğ‘›2), with â€œworst-caseâ€ omitted. Omitting the case means making a blanket statement that covers all cases, and insertion sort does not run in Î˜(ğ‘›2) time in all cases.
* You can say: The running time for insertion sort is ğ‘‚(ğ‘›2), or that itâ€™s Î©(ğ‘›), because these asymptotic running times are true for all cases.

### o-notation
* **o-notation** : characterizes a strict upper bound on the asymptotic behavior of the function
    * **O-Notation** : can be considered <=
        * ğ‘“ ğ‘› = ğ‘‚(ğ‘”(ğ‘›)): function ğ‘“ ğ‘› grows no faster than ğ‘”(n)
    * **o-Notation** : can be considered <
        * ğ‘“ ğ‘› = ğ‘‚(ğ‘”(ğ‘›)): function ğ‘“(ğ‘›) grows strictly slower than ğ‘”(n)
    * ![littleo](littleo.JPG)
### ğœ”-notation
* **ğœ”-notation** : characterizes a strict upper bound on the asymptotic behavior of the function
    * **Î© -Notation** : can be considered >=
        * ğ‘“ ğ‘› = ğ‘‚(ğ‘”(ğ‘›)): function ğ‘“ ğ‘› grows at least as fast as ğ‘”(n)
    * **ğœ”-Notation** : can be considered >
        * ğ‘“ ğ‘› = ğ‘‚(ğ‘”(ğ‘›)): function ğ‘“(ğ‘›) grows strictly faster than ğ‘”(n)
    * ![need to airdrop this image here from the phone]()
### Comparison of functions
* transitive
* symmetric
* reflexive
* transpose symmetry

### dichotomy property
* comparison between two functions f and g and the comparison of two real numbers
* ![compare](comparingNum.png)
* for real numbers either a <= b or a >= b

### Growth of functions
* ![runtime](runtimeDiag.png)
* base of a log doesnt matter asymptotically but the base of exponential and the degree of a polynomail or polylog do matter asymptotically
* ![logProps](logProps.png)
* lg = log base 2
* 2^lg(n) = n
* (sqrt(2))^lg(n) = sqrt(n)
**order of growth highest to lowest**
* n! e^n 2^n n^3 n^2 nlg(n) { 2^lgn n } (sqrt(2))^lgn) lg^2(n)

--------------------------
# Wednesday January 31st
--------------------------
# Recurrence
--------------------------
![tailopez](knowledge-tai-lopez.gif)

### Conventions
* dont need to state explicit base cases
![convention1](recurrenceConventino.png)
* state asymptotic notaiton without the ceilings and floors

#### Four methods for solving recurrences
* **substitution method** : guess the soltuiuonthen use induction to prove the solution is correct
* **recursion-tree method** : Draw out a recursion tree,
determine the cost at each level, and sum them up. This is useful for understanding of the recurrence and coming up with a guess for the substitution method
* **Master Method** :  A cookbook method for solving recurrence of the form ğ‘‡(ğ‘›) = ğ‘ğ‘‡(ğ‘›/ğ‘) + ğ‘“(ğ‘›), where ğ‘ > 0 and ğ‘ > 1 are constants, subject to certain conditions
* PROOFS on slides (idk if we'll need to know those he said at the beginning that we didn't
![recursionTreeMethod](recursionTreeMethod.JPG)
![masterMethod](MasterMethod.png)
![masterCookbook](masterCookbook.png)
# THIS IS TEST CONTENT ^^^^^^^
## MEMORIZE THE THREE FORMS ANALYZE THE CASE AND APPLY IT DIRECTLY
    for the master method the driving function is the f(n) term and the watershed function is the n^(logb(a)) fuction ==> in each of the cass you are comparing the watershed function to the driving function
    the final answer is givin by the f(n) however, you have to increase the polylog function by one
    if f(n) = n^2/(lg(n)) then the power of the lg function is -1 so case 2 doesnt apply and furhter the master method doesnt apply
![mmEx1](mmEx1.png)
![mmEx1](mmEx2.png)
![mmEx1](mmEx3.png)
![mmEx1](mmEx4.png)

--------------------------
# Friday Feburary 2nd
--------------------------
# Divide and Conquer Algorithms
--------------------------

### Binary Search
1. **Divide** : check middle element
2. **Conquer** : Recursivelt search a subarray (left or right)
3. **Combine** : Trivial (do nothing)
* https://www.youtube.com/watch?v=fDKIpRe8GW4&ab_channel=MichaelSambol
![binaryS](binarySearch.jpg)

### Matrix Multiplication
    matrixMultiply(A,B,C,n)
        for i = 1 to n
            for j = 1 to n
                for k = 1 to n
                    cij = cij + aik * bkj
* triple for loop is the algorithm
* Î˜(n^3)
* **divide and conquer algorithm for Matrix Multiplication**
    * divide a nxn matrix into 2x2 matrix of (n/2) x (n/2) submatrices:
    * ![matmult](MatrixMult.png)
    * Recurrence: T(n) = 8 T(n/2) + /theta(1)
    * because of 8 submatrix multiplications and submatrix size of n/2 and adding 1 for dividing matries
    * with the **master method** using case 1 we know the runtime is /theta(n^3)
* **Strassen's Algorithm**
    * *idea* : reduce submatrix multiplications by using more submatrix additions because thye cost less compute power
    * similar to --> x^2 + y^2 = (x-y)(x+y)
    * Recurrence: T(n) = 7 T(n/2) + /theta(n^2)
    * watershed = n^2.807 f(n) = n^2
    * bigtheta(n^2.807)
---------------
# **Monday Feburary 5th**
---------------
# **Randomized Algorithms**
---------------

### The hiring problem
* Hiring a new person you are given one candidate each day for a total of *n* days (*n candidates*)
* If you hire you must fire the current person
* ![cost](costCand.png)
* strategy is to hire the best candidate seen so far that is better than the current employee which you must fire
* becaue you must have someone hired you will hire the first candidate interviewied.
* ![hireCode](hirePsuedo.png)
    * There are n candidates total and you hire m of them (m <= n)
    * O(ci(n) + ch(m))
        * The **total interview** cost ci(n) is fixed (because n is)
        * **hiring cost** is dependent on m
        * ![hireEx](hireEx.png)
        * You take the higher level interviewee.
    * **Worst case**
        * In reverse order so you hire all of the people (ch(n))
    * **Probabilistic Analysis**
        * make assumptions about the distributions of the input by using the average over all possible inputs (**average-case run time**)
        * all n! permutations of their ranks (1,2,3,....,4) are equally likely
        * ![pa](pa.png)
    * **Indicator Random Variable**
        * Provides a convenient method for converting between Probabilities(Pr) and expectations (E).
        * ![ira](iraProof.png)
    * **Coin Flip Problem**
        * ![coinFlip](coinFlip.png)
    * **Back to the Hiring Problem**
        * **What is the probability that candidate i is hired?**
            * For candidate i = 1,2,....,n define Xi = I{candidate is hired} as an indicator random variable
            * E[Xi] = Pr{Candidide i is hired} = 1/i
            * this means 10 candidates => 1/10 chance of gettign hired
            * The first i candidates are all equally likely to be the best candidate
        * ![hiringProb](hiringProbSummation.JPG)
    * **From Probalisitic Analysis to Randomized Algorithm**
        * **Probablisitc Analysis**
            * Assumesk knowledge of distribution input
        * **Randomized Algorithm**
            * Uses randomization to enforce a distribution on the inputs inside the algorithm instead of in the input
            * for psuedo start with randomly permute (change the order) the list of candidates
            * random permutations ensure that it is not the worst case
    *![vocab](analysisVocab.png)

---------------
# **Wednesday Feburary 7th**
---------------
# **Quick Sort**
---------------
### Mood for this lecture
 ![zukc](zuck.gif)
### We grilling some mf meats

### Quciksort
* **Quicksort** is a sorting algo that uses divide and conquer
* This 4 minute video prob better than the whole lecture
https://www.youtube.com/watch?v=Hoixgm4-P4M&ab_channel=MichaelSambol
* Quicksort uses a **PARTITION** that selects a **pivot** value **THE LAST VALUE** and rearranges the elements on either side of the partition recursively
    * ![quicksort](quicksort.png)
* **Worst Case Runtime**
    * The partitioning is **completely unbalanced**
    * each partition has 0 on one side and n-1 elements on the other
    * ![quickWorst](quickWorst.png)
    * /theta(n^2)
* **Best Case Runtime**
    * The partitioning is **completely balanced**
    * each partiion has roughly n/2 elements
    * ![quickMM](quickMM.png)
    * /theta(nlgn) ---> Case #2 of the master method
* **Average Case Runtime**
* **AVERAGE**
    * much closer to the best case
    * ![quickAvg](quickAvg.png)
    * Any split of **constant proportionality** yields a runnign time of O(nlgn) the ratio only affects the constant which has no affect asymptotically i.e. /theta

### Randomized Quicksort
* To enforce an input distribution, we can again randomize the algorithm, and get the same expected running time ğ‘‚ ğ‘› lg ğ‘›

---------------
# **Friday Feburary 9th**
---------------
# **Heapsort**
---------------
    Mood for todays heaps
![max](maxV.gif)

### Heap
* **Binary Heap** data structure is an array object which can also be viewed as *(nearly) complete binary tree*
    * Each node of the tree corresponds to an element of the array
    * It is filled on all levels except the lowest level
    * lowest level is filled from left to right
    * ![heap.png](heap.png)
* **Indexes**
    * Parent(i) = i//2
    * Left(i) = 2i
    * Right(i) = 2i+1
* **Array View**
    * Array is represneted as A[1:n] with n elements
* **Heap View**
    * Heap has an attribute A.heap-size which repreenst the number of valid elements
    * 0 < A.heap-size <= n
* **Height**
    * for mostly complete ---> Î˜(lg ğ‘›)
    * ![macHeapp](macHeapp.jpg)
* **Mn Max Heaps**
    * ![minmax.jpg](minmax.png)
* **Max Heapify**
    * Recursive procedure that maitains the amx-heap rpoperty for a heap A on index i by:
        * Binary trees rooated at Left(i) and right(i) the two childern off of the root
        * But A[i] the root may be smaller than its children thus violating the max heap property
        * ![heapify](heapify.JPG)
    * ![maaax.jpg](maaax.png)
    * **Runtime**
        * O (lgn)
        * *because it has a height of h and h =lgn but its big O because you might not always go down to a leaf*
* **Building a Heap**
    * **Max Heapify** builds a heap from the bottom up
    * ![buildHepa](buildHeap.png)
    * You start at the n//2 index because that is where the last parent node would be the higher indexes wouldnt have children (in this scenario index 5)
### Heap Sort
* ![heapsory](heapsory.png)

### Why is heapsort correct?
* ![heapsory](heapsortCorrect.png)
---------------
# **Monday Feburary 12th**
---------------
# **Started class with heapsort however, he hasnt posted the following slides**
---------------
###### *Mood today with a chiefs superbowl win*
![patM](patM.gif)
* ### Heapsort info
    * *Watch his video below for heapsort*
    * https://www.youtube.com/watch?v=2DmK_H7IdTo&ab_channel=MichaelSambol
    * ![heapRun](heapRuntime.JPG)
### Comparing Sorting Algos
* ![comparRuntime](compareRuntime.png)
### Sorting in Place
* It is called in place if the sorting algo is done *directly on input array*
    * Most in place sorting algos use Î˜(1) of additional space making them *space efficient*
* **Out of place sorting**
    * Not sorted in place
    * Î˜(n) additional space to sort an array of n elements
* ![sortPlaces](sortPlaces.png)

### Prioriy Queues
* Another popular application of heap is to use it as efficient priority queue, which is a data structure for maintaining a set ğ‘† of elements, each with an associated value called a key
* Come in two forms
    * Max-priority queues
    * Min-priority queues
### Max priorirty queue
* supports
*Underflow means that we check if the heap is empty*
    * max -> returns element with largest key
        * Runtime: Î˜(1)
        * ![maxHeapMax](maxHeapMax.png)
    * extract-max -> removes and returns item with largest key
        * Running time: ğ‘‚(lg(ğ‘›)) , since MAX-HEAPIFY take ğ‘‚(lg(ğ‘›)) time
        * ![extract](maxHeapExtract.png)
    * increase-key -> increase value of element x's key to k with k >= x
        * Running time: ğ‘‚(lg(ğ‘›)) , since the height of heap is ğ‘‚(lg(ğ‘›)) and assuming finding element ğ‘¥ in the array takes Î˜(1) time
        * ![increase](increaseKey.png)
        * ![increase](IOKey.png)
    * insert -> inserts element with key in the set
        * Running time: ğ‘‚ lg ğ‘› , since MAX-HEAP-INCREASE- KEY takes ğ‘‚ lg ğ‘› time
        * ![maxhi](maxHeapInc.png)
![maxheapsummary](maxHeapSummary.png)

### Scheduling jobs on a computer shared among multiple users.
* The max-priority queue keeps track of the jobs to be executed and their relative priorities.
* When a job is finished, the scheduler selects the highest-priority job from among those pending by calling EXTRACT-MAX.
* When a job has been pending for a long time, its priority can be increased by calling INCREASE-KEY.
* The scheduler can add a new job to the queue at any time by calling INSERT

---------------
# **Wednesday Feburary 14th**
---------------
# **Elementary Data Sets**
---------------
### Sets
* In CS sets can grow, shrink, and be manipulated
* Operations
    * Can have different types of operations such as:
    * insert, delete, search -> Sets with these operations are called **Dictionaries**
    * finding max and min values.
### Dynamic Sets
* Key
    * identifier
    * ex: student id
* Satellite Data
    * carried around
    * ex: name, gender, age
* Other data
    * ex; pointers to other objects
* Some dynamic sets assume there is a *total order* based numbers or alphabetic order often
    * This allows us to defin a min element and a follwoing element
* Data Structures
    * Arrays & matrices
    * Stacks & queues
    * Linked lists
    * Skip lists
    * (Search) Trees
    * Hash tables
    * Disjoint sets
    * Graphs
### Dynamic Set operations
* Two categories
    * Queries
        * return info about set
    * Modifying Operations
        * change info about the set
* COmmon set operations
    * search, insert, delete
    * min, max, successor, predecessor
* Time complexity
    * **If array is not sorted**
        * INSERT and DELETE take Î˜(1) time
        * SEARCH, MINIMUM, MAXIMUM) can take ğ‘‚(ğ‘›) time in the worst case
    * **If array is sorted**
        * e.g., MINIMUM, MAXIMUM) take Î˜(1) time
        * SEARCH takes ğ‘‚ lg ğ‘› using binary search, but INSERT and DELETE can take ğ‘‚(ğ‘›) time in the worst case
### Arrays
* A contingious sequence of memory space (i.e pandas in pyhton)
    * ex [1,2,3]
    * Given an array index ğ‘–, it takes Î˜(1) time to access the corresponding array element ğ´[ğ‘–]
### Matrices
* 2D+ array
[[1,2,3]
[4,5,6]]
* row major: 1,2,3,4,5,6
* column major: 1,4,2,5,3,6
* *Single vs Multiple Array Representation*
    * *single array* is more efficient on modern computer systems 
    * *multiple array* can be more flexible when rows and columns have different lengths
* takes Î˜(1) time to access the corresponding matrix element ğ‘€[ğ‘–, ğ‘—]
